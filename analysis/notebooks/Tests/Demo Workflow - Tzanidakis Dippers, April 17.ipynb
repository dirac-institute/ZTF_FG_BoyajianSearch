{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import astropy.units as u\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "rcParams['savefig.dpi'] = 550\n",
    "rcParams['font.size'] = 20\n",
    "plt.rc('font', family='serif')\n",
    "from tqdm import tqdm\n",
    "import lsdb\n",
    "import dask\n",
    "dask.config.set({\"temporary-directory\" :'/epyc/ssd/users/atzanida/tmp'})\n",
    "dask.config.set({\"dataframe.shuffle-compression\": 'Snappy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29734b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=15, memory_limit=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fgk_object = lsdb.read_hipscat(\"/nvme/users/atzanida/tmp/sample_final_starhorse_hips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgk_table = fgk_object.cone_search(ra=131.312,\n",
    "    dec=14.315,\n",
    "    radius_arcsec=1_000)\n",
    "fgk_table = fgk_table.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only RA and DEC columns\n",
    "new_fgk = fgk_table[['RA_ICRS_StarHorse', 'DE_ICRS_StarHorse', 'ps1_objid_ztf_dr14']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fgk.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_fgk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fgk.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d44550",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "hips_fgk_object = lsdb.from_dataframe(new_fgk,\n",
    "                                ra_column=\"RA_ICRS_StarHorse\", \n",
    "                                dec_column=\"DE_ICRS_StarHorse\", \n",
    "                                threshold=5_000,\n",
    "                                      lowest_order=5,\n",
    "                                      highest_order=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ZTF DR14 sources\n",
    "ztf_sources = lsdb.read_hipscat(\"/epyc/data3/hipscat/catalogs/ztf_axs/ztf_zource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_sources = hips_fgk_object.join(\n",
    "    ztf_sources, left_on=\"ps1_objid_ztf_dr14\", right_on=\"ps1_objid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295857ec",
   "metadata": {},
   "source": [
    "## Initialize TAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456099ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from tape import Ensemble, ColumnMapper\n",
    "\n",
    "# Initialize an Ensemble\n",
    "ens = Ensemble(client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d223d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnMapper Establishes which table columns map to timeseries quantities\n",
    "colmap = ColumnMapper(\n",
    "        id_col='_hipscat_index',\n",
    "        time_col='mjd',\n",
    "        flux_col='mag',\n",
    "        err_col='magerr',\n",
    "        band_col='band',\n",
    "      )\n",
    "\n",
    "ens.from_dask_dataframe(\n",
    "    source_frame=_sources._ddf,\n",
    "    object_frame=hips_fgk_object._ddf,\n",
    "    column_mapper=colmap,\n",
    "    sync_tables=False, # Avoid doing an initial sync\n",
    "    sorted=True, # If the input data is already sorted by the chosen index\n",
    "    sort=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6725321",
   "metadata": {},
   "source": [
    "## Custom Time-Series Function for TAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89ca6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import stats as astro_stats\n",
    "import astropy.units as u\n",
    "from astropy.modeling.models import Gaussian1D\n",
    "from astropy.modeling import fitting\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import interpolate\n",
    "\n",
    "from astropy.io import ascii\n",
    "from scipy.signal import find_peaks\n",
    "import scipy.integrate as integrate\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def prepare_lc(time, mag, mag_err, flag, band, band_of_study='r', flag_good=0, q=None, custom_q=False):\n",
    "    \"\"\"\n",
    "    Prepare the light curve for analysis - specifically for the ZTF data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time (array-like): Input time values.\n",
    "    mag (array-like): Input magnitude values.\n",
    "    mag_err (array-like): Input magnitude error values.\n",
    "    flag (array-like): Input flag values.\n",
    "    band (array-like): Input band values.\n",
    "    band_of_study (str): Band to study. Default is 'r' band\n",
    "    flag_good (int): Flag value for good detections. Default is 0 (see ZTF documentation)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    time (array-like): Output time values.\n",
    "    mag (array-like): Output magnitude values.\n",
    "    mag_err (array-like): Output magnitude error values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Selection and preparation of the light curve (default selection on )\n",
    "    rmv = (flag == flag_good) & (mag_err>0) & (band==band_of_study) & (~np.isnan(time)) & (~np.isnan(mag)) & (~np.isnan(mag_err)) # remove nans!\n",
    "\n",
    "    time, mag, mag_err = time[rmv], mag[rmv], mag_err[rmv]\n",
    "\n",
    "    # sort time\n",
    "    srt = time.argsort()\n",
    "\n",
    "    time, mag, mag_err = time[srt], mag[srt], mag_err[srt]\n",
    "    ts = abs(time - np.roll(time, 1)) > 1e-5\n",
    "\n",
    "    time, mag, mag_err = time[ts], mag[ts], mag_err[ts]\n",
    "\n",
    "    # Remove observations that are <0.5 day apart\n",
    "    cut_close_time = np.where(np.diff(time) < 0.5)[0] + 1\n",
    "    time, mag, mag_err  = np.delete(time, cut_close_time), np.delete(mag, cut_close_time), np.delete(mag_err, cut_close_time)\n",
    "\n",
    "    return time, mag, mag_err\n",
    "\n",
    "\n",
    "def best_peak_detector(peak_dictionary, min_in_dip=1):\n",
    "    \"\"\"Chose the best peak from the peak detector with a minimum number of detections threshold. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    peak_dictionary (dict): Dictionary of the peaks.\n",
    "    min_in_dip (int): Minimum number of detections in the dip. Default is 3 detections.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Table of the best dip properties.\n",
    "    \"\"\"\n",
    "    # unpack dictionary\n",
    "    N_peaks, dict_summary = peak_dictionary\n",
    "    \n",
    "    summary_matrix = np.zeros(shape=(N_peaks, 9)) # TODO: add more columns to this matrix\n",
    "    for i, info in enumerate(dict_summary.keys()):\n",
    "        summary_matrix[i,:] = np.array(list(dict_summary[f'{info}'].values()))\n",
    "\n",
    "    dip_table = pd.DataFrame(summary_matrix, columns=['peak_loc', 'window_start', 'window_end', 'N_1sig_in_dip', 'N_in_dip', 'loc_forward_dur', 'loc_backward_dur', 'dip_power', 'average_dt_dif'])\n",
    "\n",
    "    return dip_table\n",
    "    \n",
    "def deviation(mag, mag_err, R, S):\n",
    "    \"\"\"Calculate the running deviation of a light curve for outburst or dip detection.\n",
    "    \n",
    "    d >> 0 will be dimming\n",
    "    d << 0 (or negative) will be brightenning\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mag (array-like): Magnitude values of the light curve.\n",
    "    mag_err (array-like): Magnitude errors of the light curve.\n",
    "    R (float): Biweight location of the light curve (global).\n",
    "    S (float): Biweight scale of the light curve (global).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dev (array-like): Deviation values of the light curve.\n",
    "    \"\"\"\n",
    "    # Calculate biweight estimators\n",
    "    return (mag - R) / np.sqrt(mag_err**2 + S**2) \n",
    "\n",
    "\n",
    "def gaus(x, a, x0, sigma, ofs):\n",
    "    \"\"\"\"Calculate a simple Gaussian function with a term offset\"\"\"\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2)) + ofs\n",
    "\n",
    "def auto_fit(x, y, loc, base, return_model=False):\n",
    "    \"\"\"Perform a Gaussian function auto fitting with some lose priors.\"\"\"\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaus, \n",
    "                                x,\n",
    "                                y,\n",
    "                                p0=[1, loc, 1, base],\n",
    "                            bounds=((0.1, loc-5, 0.1, base-2),\n",
    "                                    (np.inf, loc+5, np.inf, base+2)))\n",
    "    except: # if fails return zeros...\n",
    "        popt, pcov = [0, 0, 0, 0], [0, 0, 0, 0]\n",
    "    \n",
    "    if return_model:\n",
    "        return gaus(x, *popt)\n",
    "    else:\n",
    "        return popt\n",
    "\n",
    "def fwhm_calc(pop):\n",
    "    \"\"\"Add parameters recovered\"\"\"\n",
    "    return 2.355 * pop[2] # fwhm \n",
    "\n",
    "def calc_sum_score(xdat, ydat, peak_dict, base, rms):\n",
    "    \"\"\"Calculate score\"\"\"\n",
    "    score_term = 0\n",
    "    for i in range(peak_dict[0]):\n",
    "        event = peak_dict[1][f'dip_{i}']\n",
    "        loc = event['peak_loc']\n",
    "        powr = event['dip_power']\n",
    "        Ndet = event['N_1sig_in_dip']\n",
    "        \n",
    "        fit_temrs = auto_fit(xdat, ydat,\n",
    "                             loc, base, return_model=False)\n",
    "        fwhm = fwhm_calc(fit_temrs)\n",
    "        \n",
    "        score_term += fwhm * powr * Ndet\n",
    "        \n",
    "    return (1/(peak_dict[0])) * (1/rms) * score_term\n",
    "\n",
    "\n",
    "def detect_bursts_edges(time, mag, center_time, baseline_mean, baseline_std, burst_threshold=3.0, expansion_indices=1):\n",
    "    \"\"\"\n",
    "    Detect bursts in a time series using linear interpolation. powered by GPT. \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time (array-like): Time values of the light curve.\n",
    "    mag (array-like): Magnitude values of the light curve.\n",
    "    center_time (float): Center time of the burst.\n",
    "    baseline_mean (float): Mean of the baseline.\n",
    "    baseline_std (float): Standard deviation of the baseline.\n",
    "    burst_threshold (float): Threshold for burst detection. Default is 3.0.\n",
    "    expansion_indices (int): Number of indices to expand the burst region. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    burst_start (float): Start time of the burst.\n",
    "    burst_end (float): End time of the burst.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize burst_start and burst_end\n",
    "    burst_start = burst_end = np.searchsorted(time, center_time)\n",
    "\n",
    "    # Find burst start\n",
    "    while burst_start > 0:\n",
    "        burst_start -= 1\n",
    "        if mag[burst_start] < baseline_mean + burst_threshold * baseline_std:\n",
    "            break\n",
    "\n",
    "    # Find burst end\n",
    "    while burst_end < len(time) - 1:\n",
    "        burst_end += 1\n",
    "        if mag[burst_end] < baseline_mean + burst_threshold * baseline_std:\n",
    "            break\n",
    "\n",
    "    # Expand burst region towards the beginning\n",
    "    burst_start = max(0, burst_start - expansion_indices)\n",
    "\n",
    "    # Expand burst region towards the end\n",
    "    burst_end = min(len(time) - 1, burst_end + expansion_indices)\n",
    "\n",
    "    # Final start and end\n",
    "    t_start, t_end = time[burst_start], time[burst_end]\n",
    "\n",
    "    # How many detections above 2std above the mean?\n",
    "    N_thresh_1 = len((mag[(time>t_start) & (time<t_end)]>baseline_mean + 2*baseline_std))\n",
    "\n",
    "    return t_start, t_end, abs(t_start-center_time), abs(t_end-center_time), N_thresh_1, 0, 0\n",
    "\n",
    "def peak_detector(times, dips, power_thresh=3, peak_close_rmv=15, pk_2_pk_cut=30):\n",
    "    \"\"\"\n",
    "    Run and compute dip detection algorithm on a light curve.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    times (array-like): Time values of the light curve.\n",
    "    dips (array-like): Deviation values of the light curve.\n",
    "    power_thresh (float): Threshold for the peak detection. Default is 3.\n",
    "    peak_close_rmv (float): Tolerance for removing peaks that are too close to each other. Default is 15.\n",
    "    pk_2_pk_cut (float): Minimum peak to peak separation. Default is 30 days.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    N_peaks (int): Number of peaks detected.\n",
    "    dip_summary (dict): Summary of the dip. Including the peak location, the window start and end, the number of 1 sigma detections in the dip, the number of detections in the dip, the forward and backward duration of the dip, and the dip power.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(dips)==0:\n",
    "            return 0, 0\n",
    "\n",
    "        #TODO: add smoothing savgol_filter again...\n",
    "        yht = dips\n",
    "\n",
    "        # Scipy peak finding algorithm\n",
    "        pks, _ = find_peaks(yht, height=power_thresh, distance=pk_2_pk_cut) #TODO: is 100 days peak separation too aggresive?\n",
    "\n",
    "        # Reverse sort the peak values\n",
    "        pks = np.sort(pks)[::-1]\n",
    "        \n",
    "        # Time of peaks and dev of peaks\n",
    "        t_pks, p_pks = times[pks], dips[pks]\n",
    "        \n",
    "        # Number of peaks\n",
    "        N_peaks = len(t_pks)\n",
    "        \n",
    "        dip_summary = {}\n",
    "        for i, (time_ppk, ppk) in enumerate(zip(t_pks, p_pks)):\n",
    "            #TODO: old version\n",
    "            #_edges = calc_dip_edges(times, dips, time_ppk, atol=0.2)\n",
    "            _edges = detect_bursts_edges(times, dips, time_ppk, np.nanmean(dips), np.nanstd(dips), burst_threshold=3.0, expansion_indices=1)\n",
    "            # t_start, t_end, abs(t_start-center_time), abs(t_end-center_time), N_thresh_1, 0, 0 : above. #TODO: remove this!\n",
    "            \n",
    "            dip_summary[f'dip_{i}'] = {\n",
    "                \"peak_loc\": time_ppk,\n",
    "                'window_start': _edges[0],\n",
    "                'window_end': _edges[1],\n",
    "                \"N_1sig_in_dip\": _edges[-3], # number of 1 sigma detections in the dip\n",
    "                \"N_in_dip\": _edges[-3], # number of detections in the dip\n",
    "                'loc_forward_dur': _edges[2],\n",
    "                \"loc_backward_dur\": _edges[3],\n",
    "                \"dip_power\":ppk,\n",
    "                \"average_dt_dif\": _edges[-1]\n",
    "            }\n",
    "                    \n",
    "        return N_peaks, dip_summary\n",
    "    except:\n",
    "        return 0, 0\n",
    "\n",
    "def eval_prelim(time_cat, mag_cat, mag_err_cat, flag_cat, band_cat):\n",
    "    \"\"\"Given the light curve source, compute the number of peaks found.\"\"\"\n",
    "    \n",
    "    # Digest my light curve. Select band, good detections & sort\n",
    "    time, mag, mag_err = prepare_lc(time_cat, mag_cat, mag_err_cat, flag_cat, band_cat, \n",
    "                                    band_of_study='r', flag_good=0, q=None, custom_q=False)\n",
    "    \n",
    "    # Digest my light curve. Select band, good detections & sort\n",
    "    time_g, mag_g, mag_err_g = prepare_lc(time_cat, mag_cat, mag_err_cat, flag_cat, band_cat, \n",
    "                                    band_of_study='g', flag_good=0, q=None, custom_q=False)\n",
    "    \n",
    "    if len(time)>10 and len(time_g)>10: \n",
    "        # Evaluate biweight location and scale & other obvious statistics\n",
    "        R, S = astro_stats.biweight.biweight_location(mag), astro_stats.biweight.biweight_scale(mag)\n",
    "\n",
    "        # Running deviation\n",
    "        running_deviation = deviation(mag, mag_err, R, S)\n",
    "\n",
    "        # Peak detection summary per light curve\n",
    "        peak_detections = peak_detector(time, running_deviation, power_thresh=4, peak_close_rmv=20, pk_2_pk_cut=20)\n",
    "        \n",
    "        if peak_detections[0]>0:\n",
    "            del time, mag, mag_err, time_g, mag_g, mag_err_g, R, S, running_deviation\n",
    "            return peak_detections[0]          \n",
    "    else:\n",
    "        del time, mag, mag_err, time_g, mag_g, mag_err_g\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# apply calc_biweight function\n",
    "batch_calc = ens.batch(\n",
    "    eval_prelim,\n",
    "    'mjd_ztf_zource', 'mag_ztf_zource', \n",
    "    'magerr_ztf_zource', 'catflags_ztf_zource',\n",
    "    'band_ztf_zource')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "full_comp = batch_calc.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdb_demo_true",
   "language": "python",
   "name": "lsdb_demo_true"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
